{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/miniconda/envs/SCIA/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import datasets as ds\n",
    "from collections import Counter\n",
    "\n",
    "from scripts import data\n",
    "from scripts.naive_bayes import from_scratch as naive_bayes\n",
    "from scripts.naive_bayes import scikit_learn as sk_naive_bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many splits does the dataset has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "'train'\n",
      "'test'\n",
      "'unsupervised'\n",
      "Number of splits: 3\n"
     ]
    }
   ],
   "source": [
    "splits: list[str] = ds.get_dataset_split_names('imdb')\n",
    "print('Splits:')\n",
    "for split in splits:\n",
    "    print(f'\\'{split}\\'')\n",
    "print(f'Number of splits: {len(splits)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 splits in the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "How big are these splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "'train' split size : 25000\n",
      "'test' split size : 25000\n",
      "'unsupervised' split size : 50000\n"
     ]
    }
   ],
   "source": [
    "datasets: list[ds.Dataset] = data.load_datasets(splits=splits)\n",
    "print('Dataset sizes:')\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f'\\'{splits[i]}\\' split size : {dataset.num_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What is the proportion of each class on the supervised splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised dataset sizes:\n",
      "'train'\n",
      "Class 0\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "Class 1\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "'test'\n",
      "Class 0\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "Class 1\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get only supervised datasets\n",
    "supervised_datasets: list[pd.DataFrame] = data.datasets_to_dataframes(datasets[0:2])\n",
    "\n",
    "print('Supervised dataset sizes:')\n",
    "# For each dataset, print the number of samples for each class\n",
    "for i, dataset in enumerate(supervised_datasets):\n",
    "    print(f'\\'{splits[i]}\\'')\n",
    "    print('Class 0')\n",
    "    print(dataset.where(dataset['label'] == 0).count())\n",
    "    print('Class 1')\n",
    "    print(dataset.where(dataset['label'] == 1).count())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, each class represents 50% of the supervised dataset (both in train and test samples)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/SCIA/NLP-2023/scripts/data.py:40: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 40 of the file /Users/francois.soulier/SCIA/NLP-2023/scripts/data.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  no_html = BeautifulSoup(text).get_text()\n"
     ]
    }
   ],
   "source": [
    "data.test_preprocessing(\"Hello, ,,,World!::\", \"hello world\")\n",
    "data.test_preprocessing(\"Hello,        U.S.A!\", \"hello u.s.a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the preprocessing to the `text` field of our training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/SCIA/NLP-2023/scripts/data.py:40: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  no_html = BeautifulSoup(text).get_text()\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = data.processed_dataframes(supervised_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the vocabulary and change the label type of the train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary: Counter = naive_bayes.build_vocabulary(texts_serie=train_df.text)\n",
    "train_df.label = train_df.label.astype(str)\n",
    "counter_class: pd.DataFrame = train_df.groupby(\"label\").agg({'text': naive_bayes.build_vocabulary})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier pseudo-code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./nbc.png\" width=\"30%\" height=\"20%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'From scratch' Naive Bayes classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "Make prediction on each text of the training and testing sets and store them in a `'model_result'` column in the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.label = test_df.label.astype(str)\n",
    "logprior, loglikelihood, vocabulary = naive_bayes.classifier(train_df, vocabulary, counter_class)\n",
    "\n",
    "test_classifier = lambda text : naive_bayes.test_classifier(text, logprior, loglikelihood, train_df, vocabulary)\n",
    "\n",
    "train_df[\"model_result\"] = train_df.text.apply(test_classifier)\n",
    "test_df[\"model_result\"] = test_df.text.apply(test_classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 ('From scratch' implementation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "We are now able to get the good predictions count, hence we can get an accuracy ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 89.84%\n",
      "Test accuracy: 81.18%\n"
     ]
    }
   ],
   "source": [
    "naive_bayes.display_results(train_df, \"Train\")\n",
    "naive_bayes.display_results(test_df, \"Test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/SCIA/NLP-2023/scripts/data.py:40: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 40 of the file /Users/francois.soulier/SCIA/NLP-2023/scripts/data.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  no_html = BeautifulSoup(text).get_text()\n",
      "/Users/francois.soulier/SCIA/NLP-2023/scripts/data.py:40: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  no_html = BeautifulSoup(text).get_text()\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = data.processed_dataframes(supervised_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sk_naive_bayes.pipeline()\n",
    "\n",
    "# Train the model on the training set\n",
    "pipeline.fit(train_df.text, train_df.label)\n",
    "\n",
    "# Make predictions on the train and test sets\n",
    "train_predictions = pipeline.predict(train_df.text)\n",
    "test_predictions = pipeline.predict(test_df.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (scikit-learn implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83     12500\n",
      "           1       0.86      0.76      0.80     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n",
      "Confusion matrix (testing):\n",
      "[[10969  1531]\n",
      " [ 3052  9448]]\n",
      "\n",
      "\n",
      "Training accuracy: 90.32%\n",
      "Testing accuracy: 81.67%\n"
     ]
    }
   ],
   "source": [
    "# Print the classificiation report\n",
    "print('Classification report (testing):')\n",
    "sk_naive_bayes.print_classification_report(test_df, test_predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion matrix (testing):')\n",
    "sk_naive_bayes.print_confusion_matrix(test_df, test_predictions)\n",
    "print('\\n')\n",
    "\n",
    "# Print the accuracy score (training)\n",
    "sk_naive_bayes.print_accuracy_score(train_df, train_predictions, 'Training')\n",
    "# Print the accuracy score (testing)\n",
    "sk_naive_bayes.print_accuracy_score(test_df, test_predictions, 'Testing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (FIXME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: A revoir (regarder la doc de CountVectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hyperparamètre $\\alpha$ permet de réguler le sur-apprentissage. En effet, si $\\alpha$ est trop grand, le modèle va être trop régularisé et donc ne pas être capable de prédire correctement les données. Si $\\alpha$ est trop petit, le modèle va être trop adapté aux données d'entrainement et donc ne pas être capable de prédire correctement les données de test. C'est un paramètre que l'on peut ajuster dans l'implémentation `scikit-learn`, mais pas notre propre implémentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics is a sufficient metric to measure the performance of our model. Indeed, the dataset is equally distributed between the classes and are well separated between positive and negative sentiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 (TODO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i - Highest likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the words with the highest likelihood in each class (if you use scikit-learn, you want to check feature_log_prob_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  0\n",
      "Most likely words:  ['was', 'that', 'this', 'in', 'it', 'is', 'to', 'of', 'and', 'the']\n",
      "\n",
      "Class:  1\n",
      "Most likely words:  ['as', 'this', 'that', 'it', 'in', 'is', 'to', 'of', 'and', 'the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the most likely words\n",
    "sk_naive_bayes.print_most_likely_words(pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii - Stopwords removal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/francois.soulier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 91.91%\n",
      "Testing accuracy: 82.74%\n"
     ]
    }
   ],
   "source": [
    "stop_words: list[str] = sk_naive_bayes.get_stopwords()\n",
    "pipeline = sk_naive_bayes.pipeline(stop_words=stop_words)\n",
    "\n",
    "# Train the model on the training set\n",
    "pipeline.fit(train_df.text, train_df.label)\n",
    "\n",
    "# Make predictions on the train and test sets\n",
    "train_predictions = pipeline.predict(train_df.text)\n",
    "test_predictions = pipeline.predict(test_df.text)\n",
    "\n",
    "# Print the accuracy score (training)\n",
    "sk_naive_bayes.print_accuracy_score(train_df, train_predictions, 'Training')\n",
    "# Print the accuracy score (testing)\n",
    "sk_naive_bayes.print_accuracy_score(test_df, test_predictions, 'Testing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/francois.soulier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new preprocessing function is implemented in './scripts/naive_bayes/scikit_learn.py'. The text preprocessing now includes lemmatization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (FIXME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = sk_naive_bayes.pipeline(stop_words=stop_words)\n",
    "\n",
    "# train_df = train_df.text.apply(sk_naive_bayes.preprocess)\n",
    "# test_df = test_df.text.apply(sk_naive_bayes.preprocess)\n",
    "\n",
    "# # Train the model on the training set\n",
    "# pipeline.fit(train_df.text, train_df.label)\n",
    "\n",
    "# # Make predictions on the train and test sets\n",
    "# train_predictions = pipeline.predict(train_df.text)\n",
    "# test_predictions = pipeline.predict(test_df.text)\n",
    "\n",
    "# # Print the accuracy score (training)\n",
    "# sk_naive_bayes.print_accuracy_score(train_df, train_predictions, 'Training')\n",
    "# # Print the accuracy score (testing)\n",
    "# sk_naive_bayes.print_accuracy_score(test_df, test_predictions, 'Testing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (TODO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
