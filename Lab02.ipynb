{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/miniconda/envs/SCIA/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import datasets as ds\n",
    "from collections import Counter\n",
    "\n",
    "from scripts import data\n",
    "from scripts.naive_bayes import from_scratch as naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many splits does the dataset has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "'train'\n",
      "'test'\n",
      "'unsupervised'\n",
      "Number of splits: 3\n"
     ]
    }
   ],
   "source": [
    "splits: list[str] = ds.get_dataset_split_names('imdb')\n",
    "print('Splits:')\n",
    "for split in splits:\n",
    "    print(f'\\'{split}\\'')\n",
    "print(f'Number of splits: {len(splits)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 splits in the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "How big are these splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "'train' split size : 25000\n",
      "'test' split size : 25000\n",
      "'unsupervised' split size : 50000\n"
     ]
    }
   ],
   "source": [
    "datasets: list[ds.Dataset] = data.load_datasets(splits=splits)\n",
    "print('Dataset sizes:')\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f'\\'{splits[i]}\\' split size : {dataset.num_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What is the proportion of each class on the supervised splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised dataset sizes:\n",
      "'train'\n",
      "Class 0\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "Class 1\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "'test'\n",
      "Class 0\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "Class 1\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get only supervised datasets\n",
    "supervised_datasets: list[pd.DataFrame] = data.datasets_to_dataframes(datasets[0:2])\n",
    "\n",
    "print('Supervised dataset sizes:')\n",
    "# For each dataset, print the number of samples for each class\n",
    "for i, dataset in enumerate(supervised_datasets):\n",
    "    print(f'\\'{splits[i]}\\'')\n",
    "    print('Class 0')\n",
    "    print(dataset.where(dataset['label'] == 0).count())\n",
    "    print('Class 1')\n",
    "    print(dataset.where(dataset['label'] == 1).count())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, each class represents 50% of the supervised dataset (both in train and test samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/SCIA/NLP (Non Deep)/NLP-2023/scripts/data.py:40: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 40 of the file /Users/francois.soulier/SCIA/NLP (Non Deep)/NLP-2023/scripts/data.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  no_html = BeautifulSoup(text).get_text()\n"
     ]
    }
   ],
   "source": [
    "data.test_preprocessing(\"Hello, ,,,World!::\", \"hello world\")\n",
    "data.test_preprocessing(\"Hello,        U.S.A!\", \"hello u.s.a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the preprocessing to the `text` field of our training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/miniconda/envs/SCIA/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = data.processed_dataframes(supervised_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the vocabulary and change the label type of the train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary: Counter = naive_bayes.build_vocabulary(texts_serie=train_df.text)\n",
    "train_df.label = train_df.label.astype(str)\n",
    "counter_class: pd.DataFrame = train_df.groupby(\"label\").agg({'text': naive_bayes.build_vocabulary})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier pseudo-code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./nbc.png\" width=\"30%\" height=\"20%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Naive Bayes classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data\n",
    "Make prediction on each text of the training and testing sets and store them in a `'model_result'` column in the pandas dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Remove unknown words from the test set that are not in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.label = test_df.label.astype(str)\n",
    "logprior, loglikelihood, vocabulary = naive_bayes.classifier(train_df, vocabulary, counter_class)\n",
    "\n",
    "test_classifier = lambda text : naive_bayes.test_classifier(text, logprior, loglikelihood, train_df, vocabulary)\n",
    "\n",
    "train_df[\"model_result\"] = train_df.text.apply(test_classifier)\n",
    "test_df[\"model_result\"] = test_df.text.apply(test_classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 ('From scratch' implementation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "We are now able to get the good predictions count, hence we can get an accuracy ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 89.84%\n",
      "Test accuracy: 81.18%\n"
     ]
    }
   ],
   "source": [
    "naive_bayes.display_results(train_df, \"Train\")\n",
    "naive_bayes.display_results(test_df, \"Test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (scikit-learn implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: A revoir (regarder la doc de CountVectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hyperparamètre $\\alpha$ permet de réguler le sur-apprentissage. En effet, si $\\alpha$ est trop grand, le modèle va être trop régularisé et donc ne pas être capable de prédire correctement les données. Si $\\alpha$ est trop petit, le modèle va être trop adapté aux données d'entrainement et donc ne pas être capable de prédire correctement les données de test. C'est un paramètre que l'on peut ajuster dans l'implémentation `scikit-learn`, mais pas notre propre implémentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics is a sufficient metric to measure the performance of our model. Indeed, the dataset is equally distributed between the classes and are well separated between positive and negative sentiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
