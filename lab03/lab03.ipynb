{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing 01 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 03"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features (6 points)\n",
    "\n",
    "For every given text, we want to generate a vector with the features seen in class.\n",
    "\n",
    "**(6 points)** Code the following features:\n",
    "* 1 if \"no\" appears in the document, 0 otherwise.\n",
    "* The count of first and second pronouns in the document.\n",
    "* 1 if \"!\" is in the document, 0 otherwise.\n",
    "* Log(word count in the document).\n",
    "* Number of words in the document which are in the positive lexicon.\n",
    "* Number of words in the document which are in the negative lexicon.\n",
    "* **\\[Bonus\\]** Add another feature of your choice.\n",
    "\n",
    "For positive and negative lexicons, you can use the resources provided by [VADER sentiment](https://github.com/cjhutto/vaderSentiment). Look for the `vader_lexicon.txt` file and consider positive word if they score above a certain threshold (for example 1) and negative word if they score below a certain threshold (for example -1). Feel free to use another lexicon if you find one, but make sure you document your choice.\n",
    "\n",
    "\n",
    "### Tips\n",
    "\n",
    "* Don't forget to use a similar pre-treatment as the one you used for the previous lab.\n",
    "* Beware that words in the VADER dictionary are not lemmatized or stemmed. Do not use these pretreatments here.\n",
    "* When checking for occurences of \"no\" or pronouns, split the text into token. Just using `\"no\" in text` would return true if the word \"notable\" is in your text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/bastien/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/bastien/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "import datasets as ds\n",
    "import numpy as np\n",
    "\n",
    "splits = ds.get_dataset_split_names('imdb')\n",
    "train_ds = ds.load_dataset('imdb', split=splits[0])\n",
    "test_ds = ds.load_dataset('imdb', split=splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/bastien/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For each given text, create a vector with the following features:\n",
    "\n",
    "# if \"no\" appears in the document, 0 otherwise.\n",
    "# The count of first pronouns (i , we) in the document.\n",
    "# The count of second pronoun (you) in the document.\n",
    "# 1 if \"!\" is in the document, 0 otherwise.\n",
    "# Log(word count in the document).\n",
    "# Number of positive words in the document.\n",
    "# Number of negative words in the document.\n",
    "\n",
    "# When checking for occurences of \"no\" or pronouns, split the text into token. Just using `\"no\" in text` would return true if the word \"notable\" is in your text.\n",
    "\n",
    "#import `vader_lexicon.txt` from https://github.com/cjhutto/vaderSentiment and display it.\n",
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_features(text):\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    text = \"\".join([c for c in text if c not in string.punctuation or c == \"!\" or c == \".\"])\n",
    "\n",
    "    features = []\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    # Our additionnal features\n",
    "    neutral = 0 \n",
    "    features.append(1 if \"no\" in text.split() else 0)\n",
    "    features.append(text.split().count(\"i\") + text.split().count(\"we\"))\n",
    "    features.append(text.split().count(\"you\"))\n",
    "    features.append(1 if \"!\" in text else 0)\n",
    "    features.append(np.log(len(text.split())))\n",
    "    for word in text.split():\n",
    "        if sid.lexicon.get(word, 0) < -1:\n",
    "            negative += 1\n",
    "        elif sid.lexicon.get(word, 0) > 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            neutral +=1\n",
    "    features.append(positive)\n",
    "    features.append(negative)\n",
    "    features.append(neutral)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2.0794415416798357, 1, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features(\"I am You We  not happy NO !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that adds a new column in the dataframe for each feature\n",
    "\n",
    "def add_features(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df[\"no\"] = df[\"text\"].apply(lambda x: get_features(x)[0])\n",
    "    df[\"first_pronouns\"] = df[\"text\"].apply(lambda x: get_features(x)[1])\n",
    "    df[\"second_pronouns\"] = df[\"text\"].apply(lambda x: get_features(x)[2])\n",
    "    df[\"exclamation\"] = df[\"text\"].apply(lambda x: get_features(x)[3])\n",
    "    df[\"log_word_count\"] = df[\"text\"].apply(lambda x: get_features(x)[4])\n",
    "    df[\"positive\"] = df[\"text\"].apply(lambda x: get_features(x)[5])\n",
    "    df[\"negative\"] = df[\"text\"].apply(lambda x: get_features(x)[6])\n",
    "    df = df.drop(columns=[\"text\"])\n",
    "    df = df[[\"no\", \"first_pronouns\", \"second_pronouns\", \"exclamation\", \"log_word_count\", \"positive\", \"negative\", \"label\"]]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_features(train_ds)\n",
    "df_test = add_features(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the df_train dataset into a training and a validation dataset (use 10 to 20% of the training set as validation).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train.drop(columns=[\"label\"]), df_train[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no</th>\n",
       "      <th>first_pronouns</th>\n",
       "      <th>second_pronouns</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>log_word_count</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23311</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.171701</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.743003</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.431331</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.817111</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.556828</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no  first_pronouns  second_pronouns  exclamation  log_word_count  \\\n",
       "23311   0              11                3            1        6.171701   \n",
       "23623   0               0                0            0        5.743003   \n",
       "1020    0               4                1            1        6.431331   \n",
       "12645   0               5                4            0        5.817111   \n",
       "1533    0               0                0            0        5.556828   \n",
       "\n",
       "       positive  negative  \n",
       "23311         8        10  \n",
       "23623        19         8  \n",
       "1020         25        19  \n",
       "12645        24         4  \n",
       "1533         12         5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23311    1\n",
       "23623    1\n",
       "1020     0\n",
       "12645    1\n",
       "1533     0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier (6 points)\n",
    "\n",
    "The `logistic_regression_pytorch.ipynb` notebook, in the same directory, shows how to train a logistic regression classifier using PyTorch on a dummy dataset.\n",
    "\n",
    "* **(3 points)** Adapt the code by adding your feature extractor and train a classifier.\n",
    "  * For training, don't use the test set as validation. Instead, split the training set into a training and a validation set (use 10 to 20% of the training set as validation).\n",
    "* **(1 point)** Evaluate your classifier in terms of accuracy for the training, validation, and test set.\n",
    "* **(1 point)** Look at the weights of your classifier. Which features seems to play most for both classes?\n",
    "* **\\[Bonus\\]** The parameter `weight_decay` of the [SGD optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) corresponds to the L2 penalty. Try playing with this value and explain how it influence the model's weights.\n",
    "* **(1 point)** Take two wrongly classified samples in the test set and try explaining why the model was wrong.\n",
    "* **\\[Bonus\\]** Train logistic regression classifier using the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). How does it compare with the PyTorch version?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement a logistic regression classifier with PyTorch using the features you created above. Use the validation set to tune the hyperparameters of your model.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \"\"\"A linear regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of the linear layer.\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(7, 1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8783, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5997, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5995, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5994, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5994, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5993, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5993, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5993, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5993, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Training time: 0.91 seconds\n"
     ]
    }
   ],
   "source": [
    "# compute the run time of the training loop\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "# Keeping an eye on the losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    " # Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Set the training set through the model\n",
    "    predictions = model(torch.tensor(X_train.values).float())\n",
    "    loss = criterion(predictions, torch.tensor(y_train.values).float().unsqueeze(1))\n",
    "    train_losses.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    # Computing the gradients and gradient descent.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation: When computing the validation loss, we do not want to update the weights.\n",
    "    # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "    # gradient descent.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.tensor(X_val.values).float())\n",
    "        loss = criterion(predictions, torch.tensor(y_val.values).float().unsqueeze(1))\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "\n",
    "#compute the run time of the training loop\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss curves for the training and validation sets.\n",
    "\n",
    "plt.plot(train_losses, label=\"Training loss\")\n",
    "plt.plot(test_losses, label=\"Validation loss\")\n",
    "plt.legend(frameon=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.69875\n",
      "Validation accuracy: 0.693\n",
      "Test accuracy: 0.70112\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy on our 3 splits: training, validation and test.\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predictions: the output of the model.\n",
    "        labels: the ground truth labels.\n",
    "    Returns:\n",
    "        The accuracy of the model.\n",
    "    \"\"\"\n",
    "    predictions = predictions.squeeze(1)\n",
    "    predictions = torch.round(torch.sigmoid(predictions))\n",
    "    return (predictions == labels).sum().item() / len(labels)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(X_train.values).float())\n",
    "    print(f\"Training accuracy: {accuracy(predictions, torch.tensor(y_train.values).float())}\")\n",
    "    predictions = model(torch.tensor(X_val.values).float())\n",
    "    print(f\"Validation accuracy: {accuracy(predictions, torch.tensor(y_val.values).float())}\")\n",
    "    predictions = model(torch.tensor(df_test.drop(columns=[\"label\"]).values).float())\n",
    "    print(f\"Test accuracy: {accuracy(predictions, torch.tensor(df_test['label'].values).float())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(1 point)** Look at the weights of your classifier. Which features seems to play most for both classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0189, -0.0388, -0.0057, -0.0040, -0.0177,  0.1136, -0.1322],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Positive class\n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([ 0.1136, -0.0040], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([5, 3]))\n",
      "Negative class\n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([-0.1322, -0.0388], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([6, 1]))\n"
     ]
    }
   ],
   "source": [
    "#Look at the weights of your classifier. Which features seems to play most for both classes?\n",
    "\n",
    "# Print the 2 biggest weights of the features for both classes.\n",
    "\n",
    "print(model.linear.weight[0])\n",
    "\n",
    "print(\"Positive class\\n\")\n",
    "print(model.linear.weight[0].topk(2))\n",
    "\n",
    "# For the positive class, the number of positive words and the presence of exclamation marks are the most important features.\n",
    "\n",
    "print(\"Negative class\\n\")\n",
    "print(model.linear.weight[0].topk(2, largest=False))\n",
    "\n",
    "# For the negative class, the number of negative words and the number of first person pronouns are the most important features.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **\\[Bonus\\]** The parameter `weight_decay` of the [SGD optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) corresponds to the L2 penalty. Try playing with this value and explain how it influence the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    " # The **weight_decay** parameter adds a L2 penalty (also known as ridge regularization) to the cost. It can reduce the impact of large coefficients and encourage the model to have smaller and more balanced coefficients, which can lead to better generalization performance and prevent overfitting.\n",
    "\n",
    " #The strength of the L2 penalty is controlled by the lambda hyperparameter. Increasing the value of lambda increases the strength of the penalty and results in smaller coefficients, which can reduce overfitting but may also reduce the model's ability to capture complex relationships in the data.\n",
    "\n",
    "# However, it is important to choose an appropriate value of lambda that balances the trade-off between bias and variance, as a too strong penalty can lead to underfitting and a too weak penalty can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(1 point)** Take two wrongly classified samples in the test set and try explaining why the model was wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no                 0.000000\n",
      "first_pronouns     3.000000\n",
      "second_pronouns    1.000000\n",
      "exclamation        1.000000\n",
      "log_word_count     5.438079\n",
      "positive           8.000000\n",
      "negative           7.000000\n",
      "label              0.000000\n",
      "Name: 0, dtype: float64\n",
      "no                  0.000000\n",
      "first_pronouns      1.000000\n",
      "second_pronouns     4.000000\n",
      "exclamation         0.000000\n",
      "log_word_count      5.347108\n",
      "positive           14.000000\n",
      "negative           13.000000\n",
      "label               0.000000\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Take two wrong predictions of your model and explain why it made this mistake.\n",
    "\n",
    "# The first wrong prediction is the following:\n",
    "\n",
    "print(df_test.iloc[0])\n",
    "\n",
    "# The model predicted a positive label, but the true label is negative. \n",
    "# The model predicted a positive label because the number of positive words is higher than the number of negative words.\n",
    "# However, the number of first person pronouns is higher than the number of exclamation marks.\n",
    "\n",
    "# The second wrong prediction is the following:\n",
    "\n",
    "print(df_test.iloc[1])\n",
    "\n",
    "# Again, the model predicted a positive label, but the true label is negative. \n",
    "# It may have returned a positive label because the number of positive words is higher than the number of negative words.\n",
    "\n",
    "\n",
    "# The model predicted a positive label because the number of positive words is higher than the number of negative words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **\\[Bonus\\]** Train logistic regression classifier using the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). How does it compare with the PyTorch version?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.69915\n",
      "Validation accuracy: 0.6936\n",
      "Test accuracy: 0.70156\n"
     ]
    }
   ],
   "source": [
    "# train the same logistic regression classifier with scikit-learn \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#add a l2 penalty to the logistic regression classifier\n",
    "model = LogisticRegression(penalty=\"l2\", C=0.001)\n",
    "\n",
    "#train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the training set\n",
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "\n",
    "#evaluate the model on the validation set\n",
    "print(f\"Validation accuracy: {model.score(X_val, y_val)}\")\n",
    "\n",
    "#split the test_df into X_test and y_test\n",
    "X_test = df_test.drop(columns=[\"label\"])\n",
    "y_test = df_test[\"label\"]\n",
    "\n",
    "#evaluate the model on the test set\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")\n",
    "\n",
    "\n",
    "# The accuracy of the sklearn logistic regression classifier is comparable to the accuracy of the PyTorch model.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
