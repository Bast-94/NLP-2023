{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "uz9MQhItMlOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install portalocker\n",
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "tbuazaNYMo-4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "1WUjh5NUMrSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pj7YqaiNGdUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c068b1-b563-4fea-a7c1-46a9e6d2201e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import Iterable, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/NLP/Deep/lab02/')\n",
        "\n",
        "from scripts.utils import *\n",
        "from scripts.preprocessing.positional_encoding import PositionalEncoding\n",
        "from scripts.preprocessing.token_embedding import TokenEmbedding\n",
        "from scripts.model.transformer import Seq2SeqTransformer\n",
        "from scripts.model.helper import train\n",
        "\n",
        "# Decoding\n",
        "from scripts.decoding.greedy import greedy_translate\n",
        "from scripts.decoding.top_k import translate_k\n",
        "from scripts.decoding.top_p import translate_top_p"
      ],
      "metadata": {
        "id": "NZR0x-xXGzdQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-b_Iy44E_gP"
      },
      "source": [
        "## Encoder-decoder model\n",
        "\n",
        "Today you will implement a pretty decent machine translation model using the transformer and implement several decoding strategy.\n",
        "\n",
        "###  Go through the pyTorch tutorial\n",
        "\n",
        "To start with, just follow the pyTorch [language translation with nn.Transformer and torchtext tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html).\n",
        "\n",
        "To make the code turn on Google Colab, you need to update the preinstalled version of spaCy and download the small German and English spaCy models. As pyTorch doesn't seem to maintain its tutorial with their most recent changes, you also need to install torchdata.\n",
        "```\n",
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "```\n",
        "\n",
        "As the training takes time (~20min), you can start looking at the following steps while it finishes.\n",
        "\n",
        "At training, you will encounter `TypeError: ZipperIterDataPipe instance doesn't have valid length` (pyTorch doesn't update their tutorials). A workaround can be found [here](https://github.com/pytorch/tutorials/issues/1868)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxnrkCuRGdUe"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2uES--9eE_gR"
      },
      "outputs": [],
      "source": [
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "DEVICE = get_device()\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a-ailP-GdUe"
      },
      "source": [
        "### Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "veYXdlE2E_gS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd421144-9bfa-4851-8ef4-1a17b97e770b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object yield_tokens at 0x7f0669a8bd80>\n",
            "<generator object yield_tokens at 0x7f0669928200>\n"
          ]
        }
      ],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "build_vocabulary(vocab_transform, token_transform, SRC_LANGUAGE, TGT_LANGUAGE, UNK_IDX, special_symbols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcF_j0BGE_gS"
      },
      "source": [
        "### Model\n",
        "\n",
        "The model is defined in the `scripts/model/transformer.py`. All the preprocessing layers are defined at `scripts/preprocessing/*.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpOLHMcVLavs"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent the model from looking into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let's define a function that will take care of both. The masks are implemented at `scripts/utils.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkoPqrseGdUf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NybAChyaLjVr"
      },
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also define our loss function which is the cross-entropy loss and the optimizer used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JMgsjs48LmeX"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-S73PwZLsy4"
      },
      "source": [
        "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings. We need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that can be fed directly into our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "23NUAsPeLtTN"
      },
      "outputs": [],
      "source": [
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform,\n",
        "                                               bos_idx=BOS_IDX,\n",
        "                                               eos_idx=EOS_IDX\n",
        "                                               ) # Add BOS/EOS and create tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bIXmJMELzec"
      },
      "source": [
        "Let's define training and evaluation loop that will be called for each epoch. The training and evaluation are implement at `./scripts/model/helper.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRtdkaQsL4iJ"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WZ7oyYmvL2Gq"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "#transformer.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/transformer.pt'))\n",
        "\n",
        "# train_losses, val_losses = train(transformer,\n",
        "#                                  optimizer,\n",
        "#                                  SRC_LANGUAGE,\n",
        "#                                  TGT_LANGUAGE,\n",
        "#                                  loss_fn,\n",
        "#                                  BATCH_SIZE,\n",
        "#                                  PAD_IDX,\n",
        "#                                  text_transform,\n",
        "#                                  NUM_EPOCHS,\n",
        "#                                  DEVICE)\n",
        "\n",
        "# torch.save(transformer.state_dict(), '/content/drive/MyDrive/Colab Notebooks/transformer.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WxOMfNTWL7FV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace5f478-90cc-411a-876c-a6fae8d52e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Russia cloth spoof Russia sewing Madrid Madrid Russia silhouetted Madrid Russia Madrid Madrid Russia cloth\n"
          ]
        }
      ],
      "source": [
        "print(greedy_translate(transformer,\n",
        "                       \"Eine Gruppe von Menschen steht vor einem Iglu .\",\n",
        "                       text_transform,\n",
        "                       vocab_transform,\n",
        "                       SRC_LANGUAGE,\n",
        "                       TGT_LANGUAGE,\n",
        "                       start_symbol=BOS_IDX,\n",
        "                       eos_idx=EOS_IDX,\n",
        "                       device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ziFUx8akEJ"
      },
      "source": [
        "### **(4 points)** Theoretical questions\n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "* In the positional encoding, why are we using a combination of sinus and cosinus?\n",
        "* In the `Seq2SeqTransformer` class,\n",
        "  * What is the parameter nhead for?\n",
        "  * What is the point of the `generator`?\n",
        "* Describe the goal of the `create_mask` function. Why does it handle differently the source and target masks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d9Q4tSJapAh"
      },
      "source": [
        "`In the positional encoding, why are we using a combination of sinus and cosinus ?`\n",
        "\n",
        "The use of sine and cosine functions in positional encoding is a strategy to embed the positional information of tokens in the sequence. It's important to note that the purpose of this positional encoding is to allow the model to learn to use this information, rather than explicitly tell the model the absolute position of each token.\n",
        "\n",
        "Here's a more detailed explanation of why we use a combination of sine and cosine functions:\n",
        "\n",
        "Uniqueness: Each position gets a unique positional encoding. With the combination of sine and cosine functions of different frequencies, we can represent the positional information uniquely. Moreover, these positional encodings can be learned and are able to generalize to sequence lengths longer than the ones encountered during training.\n",
        "\n",
        "Relative positions: The Transformer model needs to understand not just the absolute position of the tokens in a sequence, but also the relative positions between tokens. The sine and cosine functions have a repeating pattern, which provides a way to capture the concept of relative position. More technically, for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\n",
        "\n",
        "Continuous representation: While the absolute position of each token could be included as a simple integer (1, 2, 3, ...), this would be a large, sparse vector and might not be as effectively learned by the model. Sinusoidal functions provide a smooth, continuous representation that may be easier for the model to generalize.\n",
        "\n",
        "They don't require a lot of memory: Unlike some other methods for encoding position, the sinusoidal method doesn't require any learned parameters. This makes it efficient and scalable.\n",
        "\n",
        "The choice of sine for even indices and cosine for odd indices is somewhat arbitrary; it's just a way to get two different signals that may be easier for the model to distinguish.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzndukYDa2nm"
      },
      "source": [
        "`In the SEQ2SEQTRANSFORMER class`\n",
        "\n",
        "* What is the parameter nhead for?\n",
        "\n",
        "In the context of the Seq2SeqTransformer class and specifically the Transformer model in PyTorch, the nhead parameter stands for the number of heads in the multi-head self-attention mechanisms.\n",
        "\n",
        "The Transformer uses a mechanism called self-attention, where it calculates an attention score for each input in the context of the entire sequence. The concept of multi-head attention means that this process is not done once but multiple times in parallel, with each \"head\" potentially learning to pay attention to different aspects of the input.\n",
        "\n",
        "Each head gets a portion of the input representation, performs self-attention independently, and then the results are concatenated and linearly transformed to result in the final output.\n",
        "\n",
        "In essence, nhead controls the number of distinct representation spaces the model can learn from and can help the model capture various aspects of the input data at different levels of abstraction. It's a hyperparameter that you can tune to optimize performance.\n",
        "\n",
        "In practice, you often see nhead set to 8 or 16 in Transformer models. This means that the self-attention mechanism is applied 8 or 16 times in parallel to each input. Each of these heads may learn to pay attention to different features in the data, thus helping to improve the model's performance.\n",
        "\n",
        "* What is the point of the generator?\n",
        "\n",
        "The generator is a linear layer that maps the model's output back to the size of the vocabulary. It is applied to the output of the decoder's self-attention layer and the encoder-decoder attention layer.\n",
        "\n",
        "The purpose of this is to transform the high-dimensional encoder-decoder output into a space that matches the number of classes (i.e., the size of the target vocabulary) that the model needs to predict. This is typically done by a linear (also known as fully connected) layer.\n",
        "\n",
        "* Describe the goal of the `create_mask` function. Why does it handle differently the source and target masks?\n",
        "\n",
        "The create_mask function creates four different masks, each serving a different purpose in the transformer architecture. These masks are utilized in the transformer to prevent attention to certain tokens : \n",
        "\n",
        "* `src_mask`: This mask is a square matrix of zeros. It's designed to be applied on source sequences in the encoder. In this particular implementation, all source tokens can attend to all other tokens (there's no masking in the encoder apart from padding), hence all values in the mask are zero.\n",
        "\n",
        "* `tgt_mask`: This is a \"look-ahead\" mask or \"future\" mask for the target sequences. It's used in the decoder to prevent a token from attending to future tokens in the same sequence, which enforces the autoregressive property. This mask is generated using the generate_square_subsequent_mask function which creates a square matrix with ones below the diagonal and zeros on and above the diagonal. The ones are then replaced with zeros and the zeros with negative infinity. The negative infinity values ensure that, after applying a softmax, these positions yield a near zero attention score, effectively masking them.\n",
        "\n",
        "* `src_padding_mask` and `tgt_padding_mask`: These masks are used to prevent the model from paying attention to padding tokens in the source and target sequences, respectively. They are generated by comparing each token in the source and target sequences to the padding index (PAD_IDX); a True (or 1) is output where there's a pad token and a False (or 0) elsewhere.\n",
        "\n",
        "The reason for different handling between source and target masks is due to the distinct roles of the encoder and decoder:\n",
        "\n",
        "The encoder processes the entire input sequence at once and thus needs to mask only the padding tokens (using src_padding_mask), not future tokens, so there's no need for a look-ahead mask.\n",
        "The decoder, on the other hand, is designed to generate each token one at a time, conditioned on previous tokens. Therefore, it needs the look-ahead mask (tgt_mask) to prevent each token from seeing future tokens, and also the padding mask (tgt_padding_mask) to ignore pad tokens. This is why the target mask is more complex and created differently from the source mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXDtAOjIcEyw"
      },
      "source": [
        "### **(6 points)** Decoding functions  \n",
        "The tutorial uses a greedy approach at decoding. Implement the following variations.\n",
        "* (3 points) A top-k sampling with temperature.\n",
        "* (1 point) A top-p sampling with temperature.\n",
        "* (2 point) Play with the k, p and temperature parameters, and qualitatively compare a few (at least 3) translation samples for each approach (even the greedy one).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyEOUBPWhGPn"
      },
      "source": [
        "`Top-k sampling with temperature`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3MZmhPMxhP3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5014500-aadc-416b-d919-42bcf85e59cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Russia cloth turkeys simultaneous Madrid sewing Madrid supported Russia Madrid Russia cloth frames ducts Walkers\n"
          ]
        }
      ],
      "source": [
        "print(translate_k(transformer,\n",
        "                  \"Eine Gruppe von Menschen steht vor einem Iglu .\",\n",
        "                  text_transform,\n",
        "                  vocab_transform,\n",
        "                  SRC_LANGUAGE,\n",
        "                  TGT_LANGUAGE,\n",
        "                  start_symbol=BOS_IDX,\n",
        "                  eos_idx=EOS_IDX,\n",
        "                  k=4,\n",
        "                  temperature=0.1,\n",
        "                  device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SYgEN-iPL_"
      },
      "source": [
        "Top-k sampling introduces an element of randomness in the generation process which leads to different outputs even for the same input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQXLo1gYimHs"
      },
      "source": [
        "`Top-p sampling with temperature`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_BMeRbebhugs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f65d49-ad0f-4e71-af30-5fd805b42245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LEGO fielding ice Javelin huts bigger staff pharmacy labor materials turkeys postcards loading trolleys kicking\n"
          ]
        }
      ],
      "source": [
        "print(translate_top_p(transformer,\n",
        "                      \"Eine Gruppe von Menschen steht vor einem Iglu .\",\n",
        "                      text_transform,\n",
        "                      vocab_transform,\n",
        "                      SRC_LANGUAGE,\n",
        "                      TGT_LANGUAGE,\n",
        "                      start_symbol=BOS_IDX,\n",
        "                      eos_idx=EOS_IDX,\n",
        "                      p=0.9,\n",
        "                      temperature=0.2,\n",
        "                      device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WIxXZWWLp32"
      },
      "source": [
        "#### Parameter search (k, p, temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UAIXoKpMcL-"
      },
      "source": [
        "Let us see the results of the decoding function with the parameter k in top-k sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bOqLjeUsMUqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26b9424-1590-4407-ed6c-c41914f1c286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "k: 2\n",
            "Generated translation:  rosey Russia cloth cloth Madrid Madrid Madrid Madrid Madrid Madrid Russia cloth Madrid Russia cloth cloth cloth\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  bee Madrid cloth cloth spoof Russia Madrid Russia cloth cloth Madrid Russia cloth turkeys spoof Russia sewing Russia\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  Russia cloth Madrid spoof Madrid Madrid Madrid Madrid Russia cloth Madrid Madrid Madrid Madrid cloth cloth sleigh Russia\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "k: 5\n",
            "Generated translation:  Russia cloth spoof Madrid sewing Russia Madrid Madrid Russia cloth Madrid Russia Madrid turkeys sewing turkeys sewing\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  bee ducts murals cloth spoof Madrid bee Madrid Russia sewing Russia cloth frames Madrid Russia cloth frames sewing\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  Russia Madrid Madrid spoof murals Madrid Madrid supported sewing Russia sewing cloth frames cloth frames cloth sleigh pointy\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "k: 10\n",
            "Generated translation:  simultaneous Madrid cloth Russia spoof Russia Madrid cloth Madrid Madrid Madrid supported sewing Madrid ducts Russia cloth\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  rosey Madrid cloth murals bee Madrid Russia Madrid Russia cloth supported Russia cloth sewing bee bee Russia Russia\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  ducts expectantly expectantly sandals Madrid supported Madrid sewing cloth cloth swords Russia Russia Russia sewing cloth cloth sewing\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "samples = []\n",
        "for (src, tgt) in test_iter: \n",
        "  if len(samples) == 3:\n",
        "    break\n",
        "  samples.append([src, tgt])\n",
        "\n",
        "samples = np.array(samples)\n",
        "\n",
        "k_values: list[int] = [2, 5, 10]\n",
        "\n",
        "generated_translation_samples = []\n",
        "for k in k_values:\n",
        "  translation_list = []\n",
        "  for sample in samples[:, 0]:\n",
        "    translation = translate_k(transformer, str(sample), text_transform, vocab_transform, SRC_LANGUAGE, TGT_LANGUAGE, start_symbol=BOS_IDX, eos_idx=EOS_IDX, k=k, temperature=0.1, device=DEVICE)\n",
        "    translation_list.append(translation)\n",
        "  generated_translation_samples.append(translation_list)\n",
        "\n",
        "for i in range(len(generated_translation_samples)):\n",
        "  print('\\n')\n",
        "  print(f'k: {k_values[i]}')\n",
        "  for j in range(len(generated_translation_samples[i])):\n",
        "    print(f'Generated translation: {generated_translation_samples[i][j]}')\n",
        "    print(f'Ground truth: {samples[j][1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsLCdmIuEUVV"
      },
      "source": [
        "In our top-k sampling, we can see that there is alway a part of the sentence that is correct and consitent. However, we can observe that there is a form of confusion with verbs. For instance, an equivalent of 'to stare at' coud not be found by the transformer. Indeed, the lack of combination of a verb and a preposition create unconsistency in the translation. Besides, the name <i>Boston Terrier</i> is not properly translated. It is always translated using adjectives and common nouns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfY-1nLcG7Ca"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "samples = []\n",
        "for (src, tgt) in test_iter: \n",
        "  if len(samples) == 3:\n",
        "    break\n",
        "  samples.append([src, tgt])\n",
        "\n",
        "samples = np.array(samples)\n",
        "\n",
        "p_values: list[int] = [0.25, 0.5, 0.85]\n",
        "\n",
        "generated_translation_samples = []\n",
        "for p in p_values:\n",
        "  translation_list = []\n",
        "  for sample in samples[:, 0]:\n",
        "    translation = translate_top_p(transformer, str(sample), p=p, temperature=0.2)\n",
        "    translation_list.append(translation)\n",
        "  generated_translation_samples.append(translation_list)\n",
        "\n",
        "for i in range(len(generated_translation_samples)):\n",
        "  print(f'p: {p_values[i]}')\n",
        "  for j in range(len(generated_translation_samples[i])):\n",
        "    print(f'Generated translation: {generated_translation_samples[i][j]}')\n",
        "    print(f'Ground truth: {samples[j][1]}')\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_cCvHr1hgWw"
      },
      "source": [
        "Using top-p sampling, we obtain similar results, we the same mistakes as with top-k sampling. As we use the same temperature in both methods, we could wonder whether the temperature the most impactful factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u6Owfl37iWHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73740ee-09c1-4eaf-ee28-345995715714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temperature: 0.1\n",
            "Generated translation:  bee bullhorn spoof frames firetrucks sewing Russia brief Madrid bee bullhorn ducts murals hospital patiently dentist stockings\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  customs traveler Russia Madrid postcards expectantly teenage bee sleigh ducts Fasteners dingy turkeys tuxedos flower outs line rasta\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  silhouetted it groceries turkeys sparks teeter Madrid sewing dingy garter Russia Patrick inspects ducts heading quarter ducts Walkers\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "temperature: 0.3\n",
            "Generated translation:  Radeo Kids product challenge irritated sparks connecting radars footballer mermaid priest piglet bazaar sorts gingerly cooktop Para\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  daytime hydrant scarfs Candy bodyless taped battling t mallet abstract Flag emerging both Eddies curious laboratory dread Kid\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  post Jumps amongst bearer moor Bienvenue texas Gentleman Ariel Happy hot trains over person Older Kayaking Dark Nine\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "temperature: 0.6\n",
            "Generated translation:  sandy sandwiches communication Marathon barely squeals Artwork Whatever scrape volleys kind luxury Apple tradition attaching sending groomsmen\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  equipment platform cause only hovering sniffs blush Wood tights jockey Dozens fell satchel more kickboxing metro motorized South\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  don guitars brush brunette handling bug entertainment Prayer jeans olive Nathalie Avril counting sandstorm oddly airplane green Hikers\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "temperature: 0.9\n",
            "Generated translation:  cone coaster try There Canyon patrol Jockeys SUCKS paining shish scenery culture answers natural Sledding hurt Ebay\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  Uniformed dead stair intersection tatooed theme constructor raping theme vacation intense executive cheer legged convenience hiding horsemen enjoying\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  Buster chairlift prop judges attraction capital Fancy freckles blush pasture mascot leads jetty troupe booth beaters Sailboats thanking\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "samples = []\n",
        "for (src, tgt) in test_iter: \n",
        "  if len(samples) == 3:\n",
        "    break\n",
        "    \n",
        "  samples.append([src, tgt])\n",
        "\n",
        "samples = np.array(samples)\n",
        "\n",
        "temperatures: list[int] = [0.1, 0.3, 0.6, 0.9]\n",
        "\n",
        "generated_translation_samples = []\n",
        "for temperature in temperatures:\n",
        "  translation_list = []\n",
        "  for sample in samples[:, 0]:\n",
        "    translation = translate_top_p(transformer, str(sample), text_transform, vocab_transform, SRC_LANGUAGE, TGT_LANGUAGE, start_symbol=BOS_IDX, eos_idx=EOS_IDX, p=0.9, temperature=temperature, device=DEVICE)\n",
        "    translation_list.append(translation)\n",
        "  generated_translation_samples.append(translation_list)\n",
        "\n",
        "for i in range(len(generated_translation_samples)):\n",
        "  print(f'temperature: {temperatures[i]}')\n",
        "  for j in range(len(generated_translation_samples[i])):\n",
        "    print(f'Generated translation: {generated_translation_samples[i][j]}')\n",
        "    print(f'Ground truth: {samples[j][1]}')\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1lJvextjOI4"
      },
      "source": [
        "We can see here that we do not necessarily obtain better results than previously, as we already used a minimal value for the temperature. However, the temperature does change the model output considerably. The higher the temperature, the less is the generated translation semantically correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y79OCOe4kOBb"
      },
      "source": [
        "### **(2 points)** Compute the BLEU score of the model\n",
        "\n",
        "Use the [sacreBLEU](https://github.com/mjpost/sacreBLEU) implementation to evaluate your model and quantitatively compare the 3 implemented decoding approaches **on the full test set**. Explain what all the output values mean (when using the `corpus_score` function).\n",
        "\n",
        "In the [python section](https://github.com/mjpost/sacrebleu#using-sacrebleu-from-python), you'll notice the library accepts more than just one possible translation as reference, but the given dataset only has one translation per sample.\n",
        "\n",
        "Using the `translate` function provided in the tutorial is pretty slow, as it translate text by text. It's recommended you modify the function to accept a list of texts as input, and batch them for translations (also **bonus point**).\n",
        "\n",
        "**\\[Bonus\\]** Use part of the test set to perform an hyperparameters search on the value of temperature, k, and p. Note that, normally, this should be done on a validation set, not the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8LQ7Vesr0kz"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "# load the test set\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# get the references\n",
        "refs = []\n",
        "for (src, tgt) in test_iter:\n",
        "        refs.append([tgt])\n",
        "\n",
        "# get the predictions\n",
        "sys = []\n",
        "for (src, tgt) in test_iter:\n",
        "        sys.append(translate(transformer, src))\n",
        "\n",
        "# compute BLEU score\n",
        "bleu = BLEU()\n",
        "bleu_score = bleu.corpus_score(sys, refs)\n",
        "\n",
        "print(bleu_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Y0e2i2dF30"
      },
      "outputs": [],
      "source": [
        "# Bleu score for top-k sampling\n",
        "\n",
        "# load the test set\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# get the references\n",
        "refs_topk = []\n",
        "for (src, tgt) in test_iter:\n",
        "    refs_topk.append([tgt])  # tgt should be a string\n",
        "\n",
        "sys_topk = []\n",
        "for (src,tgt) in test_iter:\n",
        "    translation = translate_k(transformer, src, k=10, temperature=0.1)\n",
        "    sys_topk.append(translation)  # translation is a string\n",
        "\n",
        "# compute BLEU score\n",
        "bleu = BLEU()\n",
        "bleu_score = bleu.corpus_score(sys_topk, refs_topk)\n",
        "\n",
        "print(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0ldDG8moeAi"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "# Bleu score for top-p sampling\n",
        "\n",
        "# get the references\n",
        "refs_top_p = []\n",
        "for (src, tgt) in test_iter:\n",
        "    refs_top_p.append([tgt])  \n",
        "\n",
        "sys_top_p = []\n",
        "for (src,tgt) in test_iter:\n",
        "    translation = translate_top_p(transformer, src, p=0.9, temperature=1.0)\n",
        "    sys_top_p.append(translation)\n",
        "\n",
        "# compute BLEU score\n",
        "bleu = BLEU()\n",
        "bleu_score_top_p = bleu.corpus_score(sys_top_p, refs_top_p)\n",
        "\n",
        "print(bleu_score_top_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb-vSSgMaZ_p"
      },
      "source": [
        "The output obtained is a result of the BLEU (Bilingual Evaluation Understudy) metric, which is a method for evaluating the quality of text that has been machine-translated from one language to another. BLEU scores are based on the comparison between the machine-generated translation and one or more human-created reference translations.\n",
        "\n",
        "Here's an explanation of each output value:\n",
        "\n",
        "- `BLEU = 51.57`: This is the final BLEU score. The BLEU score can range from 0 to 100. A higher score indicates a greater match with the reference translation. A score of 100 indicates a perfect match with the reference.\n",
        "\n",
        "- `90.9/70.0/44.4/25.0`: These are the scores of the n-gram precisions for n=1, 2, 3, and 4 respectively. These scores show the precision of individual n-grams in the translation. For example, 90.9 is the precision of unigrams (individual words), 70.0 is the precision of bigrams (pairs of words), and so on.\n",
        "\n",
        "- `(BP = 1.000)`: BP stands for \"Brevity Penalty\". This is a penalty applied if the produced translation is shorter than the reference. If the translation is the same length as the reference or longer, the brevity penalty is 1. If the translation is shorter, the brevity penalty is less than 1.\n",
        "\n",
        "- `ratio = 1.000`: This is the ratio of the length of the translation to the length of the reference. A ratio of 1 means the translation and the reference are of the same length.\n",
        "\n",
        "- `hyp_len = 11`: This is the length of the machine-generated translation (hypothesis).\n",
        "\n",
        "- `ref_len = 11`: This is the length of the reference translation.\n",
        "\n",
        "Note that all these values are calculated for a single translation evaluation. BLEU scores are usually calculated over a large number of translations to get a general evaluation of a machine's translation quality."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}