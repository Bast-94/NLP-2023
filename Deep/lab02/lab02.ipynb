{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/francois.soulier/miniconda/envs/SCIA/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import Iterable, List\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "from scripts.utils import *\n",
        "from scripts.preprocessing.positional_encoding import PositionalEncoding\n",
        "from scripts.preprocessing.token_embedding import TokenEmbedding\n",
        "from scripts.model.transformer import Seq2SeqTransformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-b_Iy44E_gP"
      },
      "source": [
        "## Encoder-decoder model\n",
        "\n",
        "Today you will implement a pretty decent machine translation model using the transformer and implement several decoding strategy.\n",
        "\n",
        "###  Go through the pyTorch tutorial\n",
        "\n",
        "To start with, just follow the pyTorch [language translation with nn.Transformer and torchtext tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html).\n",
        "\n",
        "To make the code turn on Google Colab, you need to update the preinstalled version of spaCy and download the small German and English spaCy models. As pyTorch doesn't seem to maintain its tutorial with their most recent changes, you also need to install torchdata.\n",
        "```\n",
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "```\n",
        "\n",
        "As the training takes time (~20min), you can start looking at the following steps while it finishes.\n",
        "\n",
        "At training, you will encounter `TypeError: ZipperIterDataPipe instance doesn't have valid length` (pyTorch doesn't update their tutorials). A workaround can be found [here](https://github.com/pytorch/tutorials/issues/1868)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s38uJS3r04RH"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install portalocker\n",
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2uES--9eE_gR"
      },
      "outputs": [],
      "source": [
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "DEVICE = get_device()\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "veYXdlE2E_gS"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "build_vocabulary(vocab_transform, token_transform, SRC_LANGUAGE, TGT_LANGUAGE, UNK_IDX, special_symbols)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jcF_j0BGE_gS"
      },
      "source": [
        "### Model\n",
        "\n",
        "The model is defined in the `scripts/model/transformer.py`. All the preprocessing layers are defined at `scripts/preprocessing/*.py`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YpOLHMcVLavs"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent the model from looking into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let's define a function that will take care of both. The masks are implemented at `scripts/utils.py`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NybAChyaLjVr"
      },
      "source": [
        "Let's now define the parameters of our model and instantiate the same. Below, we also define our loss function which is the cross-entropy loss and the optimizer used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JMgsjs48LmeX"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A-S73PwZLsy4"
      },
      "source": [
        "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings. We need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that can be fed directly into our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "23NUAsPeLtTN"
      },
      "outputs": [],
      "source": [
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform,\n",
        "                                               bos_idx=BOS_IDX,\n",
        "                                               eos_idx=EOS_IDX\n",
        "                                               ) # Add BOS/EOS and create tensor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3bIXmJMELzec"
      },
      "source": [
        "Let's define training and evaluation loop that will be called for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YkdSocRALvtg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, device=DEVICE)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, device=DEVICE)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FRtdkaQsL4iJ"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ7oyYmvL2Gq",
        "outputId": "87b47e3c-bc6c-4e96-e46d-b262e1cb9099"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "#transformer.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/transformer.pt'))\n",
        "\n",
        "# for epoch in range(1, NUM_EPOCHS+1):\n",
        "#     start_time = timer()\n",
        "#     train_loss = train_epoch(transformer, optimizer)\n",
        "#     end_time = timer()\n",
        "#     val_loss = evaluate(transformer)\n",
        "#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "# torch.save(transformer.state_dict(), '/content/drive/MyDrive/Colab Notebooks/transformer.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Scz0B71ulIuF"
      },
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0), device=DEVICE)\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxOMfNTWL7FV",
        "outputId": "a0a5cab4-70b5-4bf2-dd7e-9bf452b3016d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Russia cloth spoof Russia sewing Madrid Madrid Russia silhouetted Madrid Russia Madrid Madrid Russia cloth\n"
          ]
        }
      ],
      "source": [
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "62ziFUx8akEJ"
      },
      "source": [
        "### **(4 points)** Theoretical questions\n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "* In the positional encoding, why are we using a combination of sinus and cosinus?\n",
        "* In the `Seq2SeqTransformer` class,\n",
        "  * What is the parameter nhead for?\n",
        "  * What is the point of the `generator`?\n",
        "* Describe the goal of the `create_mask` function. Why does it handle differently the source and target masks?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0d9Q4tSJapAh"
      },
      "source": [
        "`In the positional encoding, why are we using a combination of sinus and cosinus ?`\n",
        "\n",
        "The use of sine and cosine functions in positional encoding is a strategy to embed the positional information of tokens in the sequence. It's important to note that the purpose of this positional encoding is to allow the model to learn to use this information, rather than explicitly tell the model the absolute position of each token.\n",
        "\n",
        "Here's a more detailed explanation of why we use a combination of sine and cosine functions:\n",
        "\n",
        "Uniqueness: Each position gets a unique positional encoding. With the combination of sine and cosine functions of different frequencies, we can represent the positional information uniquely. Moreover, these positional encodings can be learned and are able to generalize to sequence lengths longer than the ones encountered during training.\n",
        "\n",
        "Relative positions: The Transformer model needs to understand not just the absolute position of the tokens in a sequence, but also the relative positions between tokens. The sine and cosine functions have a repeating pattern, which provides a way to capture the concept of relative position. More technically, for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\n",
        "\n",
        "Continuous representation: While the absolute position of each token could be included as a simple integer (1, 2, 3, ...), this would be a large, sparse vector and might not be as effectively learned by the model. Sinusoidal functions provide a smooth, continuous representation that may be easier for the model to generalize.\n",
        "\n",
        "They don't require a lot of memory: Unlike some other methods for encoding position, the sinusoidal method doesn't require any learned parameters. This makes it efficient and scalable.\n",
        "\n",
        "The choice of sine for even indices and cosine for odd indices is somewhat arbitrary; it's just a way to get two different signals that may be easier for the model to distinguish.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GzndukYDa2nm"
      },
      "source": [
        "`In the SEQ2SEQTRANSFORMER class`\n",
        "\n",
        "* What is the parameter nhead for?\n",
        "\n",
        "In the context of the Seq2SeqTransformer class and specifically the Transformer model in PyTorch, the nhead parameter stands for the number of heads in the multi-head self-attention mechanisms.\n",
        "\n",
        "The Transformer uses a mechanism called self-attention, where it calculates an attention score for each input in the context of the entire sequence. The concept of multi-head attention means that this process is not done once but multiple times in parallel, with each \"head\" potentially learning to pay attention to different aspects of the input.\n",
        "\n",
        "Each head gets a portion of the input representation, performs self-attention independently, and then the results are concatenated and linearly transformed to result in the final output.\n",
        "\n",
        "In essence, nhead controls the number of distinct representation spaces the model can learn from and can help the model capture various aspects of the input data at different levels of abstraction. It's a hyperparameter that you can tune to optimize performance.\n",
        "\n",
        "In practice, you often see nhead set to 8 or 16 in Transformer models. This means that the self-attention mechanism is applied 8 or 16 times in parallel to each input. Each of these heads may learn to pay attention to different features in the data, thus helping to improve the model's performance.\n",
        "\n",
        "* What is the point of the generator?\n",
        "\n",
        "The generator is a linear layer that maps the model's output back to the size of the vocabulary. It is applied to the output of the decoder's self-attention layer and the encoder-decoder attention layer.\n",
        "\n",
        "The purpose of this is to transform the high-dimensional encoder-decoder output into a space that matches the number of classes (i.e., the size of the target vocabulary) that the model needs to predict. This is typically done by a linear (also known as fully connected) layer.\n",
        "\n",
        "* Describe the goal of the `create_mask` function. Why does it handle differently the source and target masks?\n",
        "\n",
        "The create_mask function creates four different masks, each serving a different purpose in the transformer architecture. These masks are utilized in the transformer to prevent attention to certain tokens : \n",
        "\n",
        "* `src_mask`: This mask is a square matrix of zeros. It's designed to be applied on source sequences in the encoder. In this particular implementation, all source tokens can attend to all other tokens (there's no masking in the encoder apart from padding), hence all values in the mask are zero.\n",
        "\n",
        "* `tgt_mask`: This is a \"look-ahead\" mask or \"future\" mask for the target sequences. It's used in the decoder to prevent a token from attending to future tokens in the same sequence, which enforces the autoregressive property. This mask is generated using the generate_square_subsequent_mask function which creates a square matrix with ones below the diagonal and zeros on and above the diagonal. The ones are then replaced with zeros and the zeros with negative infinity. The negative infinity values ensure that, after applying a softmax, these positions yield a near zero attention score, effectively masking them.\n",
        "\n",
        "* `src_padding_mask` and `tgt_padding_mask`: These masks are used to prevent the model from paying attention to padding tokens in the source and target sequences, respectively. They are generated by comparing each token in the source and target sequences to the padding index (PAD_IDX); a True (or 1) is output where there's a pad token and a False (or 0) elsewhere.\n",
        "\n",
        "The reason for different handling between source and target masks is due to the distinct roles of the encoder and decoder:\n",
        "\n",
        "The encoder processes the entire input sequence at once and thus needs to mask only the padding tokens (using src_padding_mask), not future tokens, so there's no need for a look-ahead mask.\n",
        "The decoder, on the other hand, is designed to generate each token one at a time, conditioned on previous tokens. Therefore, it needs the look-ahead mask (tgt_mask) to prevent each token from seeing future tokens, and also the padding mask (tgt_padding_mask) to ignore pad tokens. This is why the target mask is more complex and created differently from the source mask."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JXDtAOjIcEyw"
      },
      "source": [
        "### **(6 points)** Decoding functions  \n",
        "The tutorial uses a greedy approach at decoding. Implement the following variations.\n",
        "* (3 points) A top-k sampling with temperature.\n",
        "* (1 point) A top-p sampling with temperature.\n",
        "* (2 point) Play with the k, p and temperature parameters, and qualitatively compare a few (at least 3) translation samples for each approach (even the greedy one).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JyEOUBPWhGPn"
      },
      "source": [
        "`Top-k sampling with temperature`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lCgDPG8ZhL_N"
      },
      "outputs": [],
      "source": [
        "def top_k_sampling_decode(model, src, src_mask, max_len, start_symbol, k=10, temperature=1.0):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0), device=DEVICE)\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        logits = model.generator(out[:, -1])\n",
        "        \n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "        \n",
        "        # Top-k sampling\n",
        "        indices_to_remove = logits < torch.topk(logits, k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = float('-inf')\n",
        "        \n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        next_word = torch.multinomial(probs, num_samples=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate_k(model: torch.nn.Module, src_sentence: str, k=10, temperature=1.0):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = top_k_sampling_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MZmhPMxhP3b",
        "outputId": "63f71c22-eec8-496d-8803-c275e6a8ff1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " rosey Russia Russia cloth Madrid murals Madrid Russia silhouetted Russia Madrid Russia sewing Russia cloth\n"
          ]
        }
      ],
      "source": [
        "print(translate_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=4, temperature=0.1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SYgEN-iPL_"
      },
      "source": [
        "Top-k sampling introduces an element of randomness in the generation process which leads to different outputs even for the same input."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QQXLo1gYimHs"
      },
      "source": [
        "`Top-p sampling with temperature`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7vzdTstoi2rU"
      },
      "outputs": [],
      "source": [
        "def top_p_sampling_decode(model, src, src_mask, max_len, start_symbol, p=0.9, temperature=1.0):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        logits = model.generator(out[:, -1])\n",
        "\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort probabilities\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Get the smallest set of tokens whose cumulative probability exceeds p\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Create a new probabilities tensor to sample from\n",
        "        new_probs = probs.clone()\n",
        "        new_probs.squeeze()[sorted_indices[sorted_indices_to_remove]] = 0\n",
        "\n",
        "        next_word = torch.multinomial(new_probs, num_samples=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def translate_top_p(model: torch.nn.Module, src_sentence: str, p=0.9, temperature=1.0):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = top_p_sampling_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BMeRbebhugs",
        "outputId": "a88b9ee1-12ce-4bd7-beba-e3528098e2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " cloth challenge Walkers murals cloth supported Madrid Madrid Russia Broadway Walkers turkeys bullhorn patiently patiently\n"
          ]
        }
      ],
      "source": [
        "print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.4, temperature=0.1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6WIxXZWWLp32"
      },
      "source": [
        "#### Parameter search (k, p, temperature)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7UAIXoKpMcL-"
      },
      "source": [
        "Let us see the results of the decoding function with the parameter k in top-k sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOqLjeUsMUqa",
        "outputId": "9b564b13-bb3d-481b-d24a-fa29ec3ec522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "k: 2\n",
            "Generated translation:  Russia Madrid Madrid Madrid Madrid sewing cloth cloth Madrid Russia cloth Madrid Russia cloth cloth cloth Russia\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  Russia Madrid cloth spoof murals murals murals Russia Madrid Russia cloth Madrid Madrid Russia sewing cloth sewing sewing\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  Madrid Madrid Madrid spoof Madrid sewing Madrid Russia Madrid Madrid Madrid Madrid Madrid Madrid cloth cloth sleigh Russia\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "k: 5\n",
            "Generated translation:  Russia Madrid Madrid sewing Madrid cloth Madrid frames spoof Madrid supported frames Madrid turkeys bullhorn bullhorn Madrid\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  bee cloth sewing Madrid Madrid Russia supported Russia Madrid Russia Madrid sewing cloth cloth cloth Russia sleigh Russia\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  rosey cloth spoof ducts Madrid Russia sewing Madrid cloth bullhorn bullhorn Russia Madrid Russia Madrid Russia Madrid cloth\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "k: 10\n",
            "Generated translation:  bee sewing sewing sparks Russia cloth cloth silhouetted Madrid Russia Madrid supported sewing Madrid bee ducts Madrid\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  sparks sandals murals spoof Madrid supported supported sparks supported cloth Madrid Russia cloth cloth sleigh patiently sleigh cloth\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  cloth sewing ducts cloth banging sewing sewing silhouetted banging silhouetted sewing banging heading Fasteners bee patiently lensed sleigh\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "samples = []\n",
        "for (src, tgt) in test_iter: \n",
        "  if len(samples) == 3:\n",
        "    break\n",
        "  samples.append([src, tgt])\n",
        "\n",
        "samples = np.array(samples)\n",
        "\n",
        "k_values: list[int] = [2, 5, 10]\n",
        "\n",
        "generated_translation_samples = []\n",
        "for k in k_values:\n",
        "  translation_list = []\n",
        "  for sample in samples[:, 0]:\n",
        "    translation = translate_k(transformer, str(sample), k=k, temperature=0.2)\n",
        "    translation_list.append(translation)\n",
        "  generated_translation_samples.append(translation_list)\n",
        "\n",
        "for i in range(len(generated_translation_samples)):\n",
        "  print('\\n')\n",
        "  print(f'k: {k_values[i]}')\n",
        "  for j in range(len(generated_translation_samples[i])):\n",
        "    print(f'Generated translation: {generated_translation_samples[i][j]}')\n",
        "    print(f'Ground truth: {samples[j][1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XsLCdmIuEUVV"
      },
      "source": [
        "In our top-k sampling, we can see that there is alway a part of the sentence that is correct and consitent. However, we can observe that there is a form of confusion with verbs. For instance, an equivalent of 'to stare at' coud not be found by the transformer. Indeed, the lack of combination of a verb and a preposition create unconsistency in the translation. Besides, the name <i>Boston Terrier</i> is not properly translated. It is always translated using adjectives and common nouns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfY-1nLcG7Ca",
        "outputId": "47b12ee6-c6d0-48b3-86f9-d1fb17d20dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p: 0.25\n",
            "Generated translation:  burger scrape combat suspect 44 humans wok Russia alert angels shielding Ensuring fatigued swords grassy postcards snowblowing\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  pumpkins bullhorn webpage consulting humorously dirk laptop building bee turkeys sessions Fasteners thoughtfully consulting Racer okay supported pointy\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  laundromat nurse patronizing Madrid Walkers aviator camaraderie postcards Russia bunting billiard Madrid warm paining laptop decker sewing billiard\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "p: 0.5\n",
            "Generated translation:  Russia intertwining coconut plow garter chin brightly waits resembling sandals cityscape teeth nurse wait barking warm ducts\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  Musicians stream capital talent minerature accepting moor Madrid film kiteboarder snowboarder Patrick wait trekking juggle advantage peoplw Sitting\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  masks dryer challenge needing Grant peewee sewing Peterson blocks riders BUA frames sleigh burger consulting dryer announcement turntables\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "p: 0.85\n",
            "Generated translation:  trendy handgun Welcome cloth murals pastel infirmity mature swifter utensils decorating airplane school harness reclines Laying Kids\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  olympic Couple expectantly dread sculpting shoulder overnight campus postcards postcards swords colander in folders spoof grassy medatative runner\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  shielding Broadway fishbowl crystal Iran preteens lock frames waiting waters school flashes Crouching seeds bingo frolics pontoon SquarePants\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "samples = []\n",
        "for (src, tgt) in test_iter: \n",
        "  if len(samples) == 3:\n",
        "    break\n",
        "  samples.append([src, tgt])\n",
        "\n",
        "samples = np.array(samples)\n",
        "\n",
        "p_values: list[int] = [0.25, 0.5, 0.85]\n",
        "\n",
        "generated_translation_samples = []\n",
        "for p in p_values:\n",
        "  translation_list = []\n",
        "  for sample in samples[:, 0]:\n",
        "    translation = translate_top_p(transformer, str(sample), p=p, temperature=0.2)\n",
        "    translation_list.append(translation)\n",
        "  generated_translation_samples.append(translation_list)\n",
        "\n",
        "for i in range(len(generated_translation_samples)):\n",
        "  print(f'p: {p_values[i]}')\n",
        "  for j in range(len(generated_translation_samples[i])):\n",
        "    print(f'Generated translation: {generated_translation_samples[i][j]}')\n",
        "    print(f'Ground truth: {samples[j][1]}')\n",
        "  print('\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A_cCvHr1hgWw"
      },
      "source": [
        "Using top-k sampling, we obtain similar results, we the same mistakes as with top-k sampling. As we use the same temperature in both methods, we could wonder whether the temperature the most impactful factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Owfl37iWHE",
        "outputId": "3e87662b-b632-4729-81f1-be7e0b82556e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temperature: 0.1\n",
            "Generated translation:  bee Madrid cloth spoof Madrid Madrid Russia cloth Russia cloth Russia cloth turkeys sewing Russia Russia cloth\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  bee Madrid cloth spoof Russia sewing Madrid Madrid Russia cloth cloth Madrid Madrid Russia cloth cloth cloth sewing\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  rosey Russia cloth spoof Madrid Madrid Madrid Madrid Madrid Madrid Madrid Madrid Russia Madrid cloth cloth cloth cloth\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "temperature: 0.3\n",
            "Generated translation:  heading humorously spoof bullhorn sewing locksmith bee peewee bazaar riders Russia heading Walkers Middle arresting swords viewable\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  billiard Walkers contemplating frames grassy glider silhouetted coconut capital ceremonial Madrid ceremonial heading silhouetted P closely viewable laptop\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  switch Armenian squirt murals floating silhouetted humorously stool fishbowl moor phone Officers sandals ice coconut silhouetted Russia Athlete\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "temperature: 0.6\n",
            "Generated translation:  bitter rosey rosey attentively Snow lensed masks exibit Armenian stables Half ceremonial shielding armchair switch Athlete elevators\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  about dominates Korean pine shelter children Annual locksmith sparks 44 Smiling Korean cargo beaters ice league hitting pontiac\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  adjustment caressing Kok bunker welders creeping trendy plow seaside robed embankment excitingly cover protective Half billiard fork sewing\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n",
            "temperature: 0.9\n",
            "Generated translation:  prove tourist bun shinning bundt Nikon peewee equations pontiac connecting demonstration brick prestends Couple approach dingy Walmart\n",
            "Ground truth: A man in an orange hat starring at something.\n",
            "Generated translation:  chains scribe direct Brick breakdancing teeth livestock rolling odd closely winning excitingly exhausted Walkers streetlight embankment snoopy thoughtfully\n",
            "Ground truth: A Boston Terrier is running on lush green grass in front of a white fence.\n",
            "Generated translation:  turkeys self brightly patent sheath lesbians seaside seating Happy garter lantern chest naps waiting customs age cloth itself\n",
            "Ground truth: A girl in karate uniform breaking a stick with a front kick.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "samples = []\n",
        "for (src, tgt) in test_iter: \n",
        "  if len(samples) == 3:\n",
        "    break\n",
        "  samples.append([src, tgt])\n",
        "\n",
        "samples = np.array(samples)\n",
        "\n",
        "temperatures: list[int] = [0.1, 0.3, 0.6, 0.9]\n",
        "\n",
        "generated_translation_samples = []\n",
        "for temperature in temperatures:\n",
        "  translation_list = []\n",
        "  for sample in samples[:, 0]:\n",
        "    translation = translate_top_p(transformer, str(sample), p=0.1, temperature=temperature)\n",
        "    translation_list.append(translation)\n",
        "  generated_translation_samples.append(translation_list)\n",
        "\n",
        "for i in range(len(generated_translation_samples)):\n",
        "  print(f'temperature: {temperatures[i]}')\n",
        "  for j in range(len(generated_translation_samples[i])):\n",
        "    print(f'Generated translation: {generated_translation_samples[i][j]}')\n",
        "    print(f'Ground truth: {samples[j][1]}')\n",
        "  print('\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d1lJvextjOI4"
      },
      "source": [
        "We can see here that we do not necessarily obtain better results than previously, as we already used a minimal value for the temperature. However, the temperature does change the model output considerably. The higher the temperature, the less is the generated translation semantically correct."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y79OCOe4kOBb"
      },
      "source": [
        "### **(2 points)** Compute the BLEU score of the model\n",
        "\n",
        "Use the [sacreBLEU](https://github.com/mjpost/sacreBLEU) implementation to evaluate your model and quantitatively compare the 3 implemented decoding approaches **on the full test set**. Explain what all the output values mean (when using the `corpus_score` function).\n",
        "\n",
        "In the [python section](https://github.com/mjpost/sacrebleu#using-sacrebleu-from-python), you'll notice the library accepts more than just one possible translation as reference, but the given dataset only has one translation per sample.\n",
        "\n",
        "Using the `translate` function provided in the tutorial is pretty slow, as it translate text by text. It's recommended you modify the function to accept a list of texts as input, and batch them for translations (also **bonus point**).\n",
        "\n",
        "**\\[Bonus\\]** Use part of the test set to perform an hyperparameters search on the value of temperature, k, and p. Note that, normally, this should be done on a validation set, not the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8LQ7Vesr0kz",
        "outputId": "9ec22336-d1f7-4cc0-dbc6-d81a99fa7107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/francois.soulier/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m sys \u001b[39m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m (src, tgt) \u001b[39min\u001b[39;00m test_iter:\n\u001b[0;32m---> 14\u001b[0m         sys\u001b[39m.\u001b[39mappend(translate(transformer, src))\n\u001b[1;32m     16\u001b[0m \u001b[39m# compute BLEU score\u001b[39;00m\n\u001b[1;32m     17\u001b[0m bleu \u001b[39m=\u001b[39m BLEU()\n",
            "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(model, src_sentence)\u001b[0m\n\u001b[1;32m     29\u001b[0m num_tokens \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m src_mask \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mzeros(num_tokens, num_tokens))\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mbool)\n\u001b[0;32m---> 31\u001b[0m tgt_tokens \u001b[39m=\u001b[39m greedy_decode(\n\u001b[1;32m     32\u001b[0m     model,  src, src_mask, max_len\u001b[39m=\u001b[39;49mnum_tokens \u001b[39m+\u001b[39;49m \u001b[39m5\u001b[39;49m, start_symbol\u001b[39m=\u001b[39;49mBOS_IDX)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(vocab_transform[TGT_LANGUAGE]\u001b[39m.\u001b[39mlookup_tokens(\u001b[39mlist\u001b[39m(tgt_tokens\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())))\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m<bos>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m<eos>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, src, src_mask, max_len, start_symbol)\u001b[0m\n\u001b[1;32m      9\u001b[0m memory \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     10\u001b[0m tgt_mask \u001b[39m=\u001b[39m (generate_square_subsequent_mask(ys\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), device\u001b[39m=\u001b[39mDEVICE)\n\u001b[1;32m     11\u001b[0m             \u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mbool))\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 12\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(ys, memory, tgt_mask)\n\u001b[1;32m     13\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerator(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
            "File \u001b[0;32m~/SCIA/NLP/NLP-2023/Deep/lab02/scripts/model/transformer.py:50\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.decode\u001b[0;34m(self, tgt, memory, tgt_mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mdecoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositional_encoding(\n\u001b[1;32m     51\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtgt_tok_emb(tgt)), memory,\n\u001b[1;32m     52\u001b[0m                       tgt_mask)\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/transformer.py:369\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[1;32m    368\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 369\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[1;32m    370\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[1;32m    371\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[1;32m    372\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/transformer.py:717\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 717\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    718\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    720\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/transformer.py:735\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mha_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    734\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 735\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(x, mem, mem,\n\u001b[1;32m    736\u001b[0m                             attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    737\u001b[0m                             key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    738\u001b[0m                             is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[1;32m    739\u001b[0m                             need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    740\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
            "File \u001b[0;32m~/miniconda/envs/SCIA/lib/python3.10/site-packages/torch/nn/functional.py:5373\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5370\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5371\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5373\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5374\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5376\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "# load the test set\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# get the references\n",
        "refs = []\n",
        "for (src, tgt) in test_iter:\n",
        "        refs.append([tgt])\n",
        "\n",
        "# get the predictions\n",
        "sys = []\n",
        "for (src, tgt) in test_iter:\n",
        "        sys.append(translate(transformer, src))\n",
        "\n",
        "# compute BLEU score\n",
        "bleu = BLEU()\n",
        "bleu_score = bleu.corpus_score(sys, refs)\n",
        "\n",
        "print(bleu_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Y0e2i2dF30",
        "outputId": "4411c6d5-c49c-4136-bf1e-35da8b4ce8cc"
      },
      "outputs": [],
      "source": [
        "# Bleu score for top-k sampling\n",
        "\n",
        "# load the test set\n",
        "test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# get the references\n",
        "refs_topk = []\n",
        "for (src, tgt) in test_iter:\n",
        "    refs_topk.append([tgt])  # tgt should be a string\n",
        "\n",
        "sys_topk = []\n",
        "for (src,tgt) in test_iter:\n",
        "    translation = translate_k(transformer, src, k=10, temperature=0.1)\n",
        "    sys_topk.append(translation)  # translation is a string\n",
        "\n",
        "# compute BLEU score\n",
        "bleu = BLEU()\n",
        "bleu_score = bleu.corpus_score(sys_topk, refs_topk)\n",
        "\n",
        "print(bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0ldDG8moeAi",
        "outputId": "94d5e7e2-9922-463c-a2ab-0eed1f6129e4"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "# Bleu score for top-p sampling\n",
        "\n",
        "# get the references\n",
        "refs_top_p = []\n",
        "for (src, tgt) in test_iter:\n",
        "    refs_top_p.append([tgt])  \n",
        "\n",
        "sys_top_p = []\n",
        "for (src,tgt) in test_iter:\n",
        "    translation = translate_top_p(transformer, src, p=0.9, temperature=1.0)\n",
        "    sys_top_p.append(translation)\n",
        "\n",
        "# compute BLEU score\n",
        "bleu = BLEU()\n",
        "bleu_score_top_p = bleu.corpus_score(sys_top_p, refs_top_p)\n",
        "\n",
        "print(bleu_score_top_p)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb-vSSgMaZ_p"
      },
      "source": [
        "The output obtained is a result of the BLEU (Bilingual Evaluation Understudy) metric, which is a method for evaluating the quality of text that has been machine-translated from one language to another. BLEU scores are based on the comparison between the machine-generated translation and one or more human-created reference translations.\n",
        "\n",
        "Here's an explanation of each output value:\n",
        "\n",
        "- `BLEU = 51.57`: This is the final BLEU score. The BLEU score can range from 0 to 100. A higher score indicates a greater match with the reference translation. A score of 100 indicates a perfect match with the reference.\n",
        "\n",
        "- `90.9/70.0/44.4/25.0`: These are the scores of the n-gram precisions for n=1, 2, 3, and 4 respectively. These scores show the precision of individual n-grams in the translation. For example, 90.9 is the precision of unigrams (individual words), 70.0 is the precision of bigrams (pairs of words), and so on.\n",
        "\n",
        "- `(BP = 1.000)`: BP stands for \"Brevity Penalty\". This is a penalty applied if the produced translation is shorter than the reference. If the translation is the same length as the reference or longer, the brevity penalty is 1. If the translation is shorter, the brevity penalty is less than 1.\n",
        "\n",
        "- `ratio = 1.000`: This is the ratio of the length of the translation to the length of the reference. A ratio of 1 means the translation and the reference are of the same length.\n",
        "\n",
        "- `hyp_len = 11`: This is the length of the machine-generated translation (hypothesis).\n",
        "\n",
        "- `ref_len = 11`: This is the length of the reference translation.\n",
        "\n",
        "Note that all these values are calculated for a single translation evaluation. BLEU scores are usually calculated over a large number of translations to get a general evaluation of a machine's translation quality."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
