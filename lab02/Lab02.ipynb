{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import datasets as ds\n",
    "from collections import Counter\n",
    "\n",
    "from scripts import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many splits does the dataset has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:\n",
      "'train'\n",
      "'test'\n",
      "'unsupervised'\n",
      "Number of splits: 3\n"
     ]
    }
   ],
   "source": [
    "splits: list[str] = ds.get_dataset_split_names('imdb')\n",
    "print('Splits:')\n",
    "for split in splits:\n",
    "    print(f'\\'{split}\\'')\n",
    "print(f'Number of splits: {len(splits)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 splits in the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "How big are these splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/Users/francois.soulier/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "'train' split size : 25000\n",
      "'test' split size : 25000\n",
      "'unsupervised' split size : 50000\n"
     ]
    }
   ],
   "source": [
    "datasets: list[ds.Dataset] = data.load_datasets(splits=splits)\n",
    "print('Dataset sizes:')\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f'\\'{splits[i]}\\' split size : {dataset.num_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What is the proportion of each class on the supervised splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised dataset sizes:\n",
      "'train'\n",
      "Class 0\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "Class 1\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "'test'\n",
      "Class 0\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "Class 1\n",
      "text     12500\n",
      "label    12500\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get only supervised datasets\n",
    "supervised_datasets: list[pd.DataFrame] = data.datasets_to_dataframes(datasets[0:2])\n",
    "\n",
    "print('Supervised dataset sizes:')\n",
    "# For each dataset, print the number of samples for each class\n",
    "for i, dataset in enumerate(supervised_datasets):\n",
    "    print(f'\\'{splits[i]}\\'')\n",
    "    print('Class 0')\n",
    "    print(dataset.where(dataset['label'] == 0).count())\n",
    "    print('Class 1')\n",
    "    print(dataset.where(dataset['label'] == 1).count())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, each class represents 50% of the supervised dataset (both in train and test samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Create an adapted processing function which lower case the text and replace punctuations with text:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiny test (preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/SCIA/NLP (Non Deep)/NLP-2023/scripts/data.py:40: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 40 of the file /Users/francois.soulier/SCIA/NLP (Non Deep)/NLP-2023/scripts/data.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  no_html = BeautifulSoup(text).get_text()\n"
     ]
    }
   ],
   "source": [
    "data.test_preprocessing(\"Hello, ,,,World!::\", \"hello world\")\n",
    "data.test_preprocessing(\"Hello,        U.S.A!\", \"hello u.s.a\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the preprocessing to the `text` field of our training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois.soulier/miniconda/envs/SCIA/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = data.processed_dataframes(supervised_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize function\n",
    "Function to cut processed text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str)-> list:\n",
    "    \"\"\"\n",
    "    Tokenizes the given text.\n",
    "    Args:\n",
    "        text (str): Text to tokenize (pre-processed)\n",
    "    Returns:\n",
    "        list: List of tokens\n",
    "    \"\"\"\n",
    "    return [w for w in re.split(\"\\W+\", text)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe for vocabulary\n",
    "This function compute the vocabulary dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts_serie: pd.Series) -> Counter:\n",
    "    \"\"\"\n",
    "    Builds the vocabulary of the given texts serie.\n",
    "    Args:\n",
    "        text_serie (pd.Series): Text serie\n",
    "    Returns:\n",
    "        Counter: Vocabulary\n",
    "    \"\"\"\n",
    "    vocabulary: Counter = None # Use Counter as a dictionary with word occurrences\n",
    "    for text in texts_serie:\n",
    "        word_list: list[str] = tokenize(text=text)\n",
    "        if vocabulary is None:\n",
    "            vocabulary = Counter(word_list)\n",
    "        else:\n",
    "            vocabulary.update(word_list)\n",
    "    return vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the vocabulary and change the label type of the train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary: Counter = build_vocabulary(texts_serie=train_data_frame.text)\n",
    "train_data_frame.label = train_data_frame.label.astype(str)\n",
    "counter_class: pd.DataFrame = train_data_frame.groupby(\"label\").agg({'text': build_vocabulary})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which returns occurence of a word of a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(counter_class: pd.DataFrame, class_name: str, word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of occurrences of the given word in the given class.\n",
    "    Args:\n",
    "        counter_class (pd.DataFrame): DataFrame with the vocabulary of each class\n",
    "        class_name (str): Class name / label\n",
    "        word (str): Word\n",
    "    Returns:\n",
    "        int: Number of occurrences of the given word in the given class\n",
    "    \"\"\"\n",
    "    return counter_class.loc[class_name][\"text\"][word]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of naive Bayes classifier\n",
    "\n",
    "Let's implement this pseudo-code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](nbc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(vocabulary: Counter, c: str, counter_class: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Returns the total number of words in a class for the given vocabulary.\n",
    "    Args:\n",
    "        vocabulary (Counter): Vocabulary\n",
    "        c (str): Class name / label\n",
    "        counter_class (pd.DataFrame): DataFrame with the vocabulary of each class\n",
    "    Returns:\n",
    "        int: Total number of words in the given class\n",
    "    \"\"\"\n",
    "    total: int = 0\n",
    "    for w in vocabulary:\n",
    "        total += word_count(counter_class, c, w)\n",
    "    return total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which affect a specified value for loglikelihood dictionnary at index `word,class_value`. It represent the loglikelihood for a word of a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_loglikelihood(loglikelihood: dict, word: str, class_value: str, value_to_affect: float) -> None:\n",
    "    \"\"\"\n",
    "    Fills the loglikelihood dictionary with the given values.\n",
    "    Args:\n",
    "        loglikelihood (dict): Loglikelihood dictionary\n",
    "        word (str): Word\n",
    "        class_value (str): Class name / label\n",
    "        value_to_affect (float): Value to affect\n",
    "    \"\"\"\n",
    "    if (loglikelihood.get(word) is None):\n",
    "        loglikelihood[word] = {}\n",
    "    loglikelihood[word][class_value] = value_to_affect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(train_data_frame: pd.DataFrame, vocabulary: Counter, counter_class: pd.DataFrame) -> tuple[dict, dict, Counter]:\n",
    "    \"\"\"\n",
    "    Builds the Naive Bayes classifier.\n",
    "    Args:\n",
    "        train_data_frame (pd.DataFrame): Training data frame\n",
    "        vocabulary (Counter): Vocabulary\n",
    "        counter_class (pd.DataFrame): DataFrame with the vocabulary of each class\n",
    "    Returns:\n",
    "        tuple[dict, dict, Counter]: Tuple with the logprior, loglikelihood and vocabulary\n",
    "    \"\"\"\n",
    "    total_document_count: int = train_data_frame.text.count()\n",
    "    class_label_set: list = list(train_data_frame.groupby(\"label\").groups.keys())\n",
    "    logprior: dict = {}\n",
    "    loglikelihood: dict = {}\n",
    "    \n",
    "    for current_class in class_label_set:\n",
    "        class_document_count: int = train_data_frame[train_data_frame.label == current_class].text.count()\n",
    "        logprior[current_class] = np.log(class_document_count/total_document_count)\n",
    "        total: int = total_words(vocabulary,current_class,counter_class) + len(vocabulary)\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            count_w_c = word_count(counter_class, current_class, word) + 1\n",
    "            log_like_value = np.log(count_w_c / total)\n",
    "            fill_loglikelihood(loglikelihood,word,current_class,log_like_value)\n",
    "            \n",
    "    return logprior, loglikelihood, vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Naive Bayes classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the test function of our naive Bayes classifier which apply for one testdoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes_classifier(testdoc: str, logprior: dict, loglikelihood: dict, train_data_frame: pd.DataFrame, vocabulary: Counter) -> tuple:\n",
    "    \"\"\"\n",
    "    Tests the Naive Bayes classifier.\n",
    "    Args:\n",
    "        testdoc (str): Test document\n",
    "        logprior (dict): Logprior\n",
    "        loglikelihood (dict): Loglikelihood\n",
    "        train_data_frame (pd.DataFrame): Training data frame\n",
    "        vocabulary (Counter): Vocabulary\n",
    "    Returns:\n",
    "        tuple: Tuple with the predicted class and the loglikelihood\n",
    "    \"\"\"\n",
    "    class_set: list = list(train_data_frame.groupby(\"label\").groups.keys())\n",
    "    sums: dict = {}\n",
    "    max_class = None\n",
    "\n",
    "    for c in class_set:\n",
    "        sums[c] = logprior[c]\n",
    "        word_list = tokenize(testdoc)\n",
    "        for w in word_list:\n",
    "            if(vocabulary[w] != 0):\n",
    "                sums[c] = sums[c] + loglikelihood[w][c]\n",
    "\n",
    "        if (max_class is None or sums[max_class] < sums[c]):\n",
    "            max_class = c\n",
    "        \n",
    "    return max_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data\n",
    "Make prediction on each text from `test_data_frame` and store them in `model_result`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_frame.label = test_data_frame.label.astype(str)\n",
    "logprior, loglikelihood, vocabulary = naive_bayes_classifier(train_data_frame, vocabulary, counter_class)\n",
    "test_data_frame_2: pd.DataFrame = test_data_frame\n",
    "test_nbc_function = lambda text : test_naive_bayes_classifier(text, logprior,loglikelihood,train_data_frame,vocabulary)\n",
    "test_data_frame_2[\"model_result\"] = test_data_frame_2.text.apply(test_nbc_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results\n",
    "We are now able to get the good predictions count, hence we can get an accuracy ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 81.18\n"
     ]
    }
   ],
   "source": [
    "good_predictions_count: int = (test_data_frame_2[test_data_frame_2.label == test_data_frame_2.model_result]).label.count()\n",
    "text_count: int = test_data_frame_2.text.count()\n",
    "accuracy_ratio: float = good_predictions_count / text_count\n",
    "print(f'Test accuracy : {(accuracy_ratio * 100):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score(y_test, y_pred)*100 = 82.708\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "X_train = vect.fit_transform(train_data_frame.text)\n",
    "X_test = vect.transform(test_data_frame.text)\n",
    "y_train = train_data_frame.label\n",
    "y_test = test_data_frame.label\n",
    "\n",
    "clf = MultinomialNB(alpha=50.0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'{accuracy_score(y_test, y_pred)*100 = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A revoir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'hyperparamètre $\\alpha$ permet de réguler le sur-apprentissage. En effet, si $\\alpha$ est trop grand, le modèle va être trop régularisé et donc ne pas être capable de prédire correctement les données. Si $\\alpha$ est trop petit, le modèle va être trop adapté aux données d'entrainement et donc ne pas être capable de prédire correctement les données de test. C'est un paramètre que l'on peut ajuster dans l'implémentation `scikit-learn`, mais pas notre propre implémentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metrics is a sufficient metric to measure the performance of our model. Indeed, the dataset is equally distributed between the classes and are well separated between positive and negative sentiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
